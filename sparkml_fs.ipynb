{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nimport pyspark.sql.types as T\n\nspark = SparkSession.builder.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ac3b099-e456-4224-b202-ef372cc2628a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.stat import Correlation\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql import DataFrame\nimport numpy as np\n\n\ndef correlation_matrix(df: DataFrame, feature_columns: list, output_column: str) -> np.ndarray:\n    \"\"\"\n    generates the Pearson correlation coefficients matrix for feature ranking\n    \"\"\"\n\n    vec_assembler = VectorAssembler(inputCols=feature_columns + [output_column], outputCol=\"all_columns\")\n    df_vectorized = vec_assembler.transform(df)\n    cor_matrix = Correlation.corr(df_vectorized, \"all_columns\").collect()[0][0].toArray()\n    return cor_matrix\n\n\ndef feature_ranker(df: DataFrame, feature_columns: list, output_column: str, must_include_features: list = None):\n    \"\"\"\n    ranks features based on their correlation with the output and their inter-correlations\n    \"\"\"\n\n    cor_matrix = abs(correlation_matrix(df, feature_columns, output_column))\n\n    if must_include_features:\n        must_include_feature_ids = [i for i in range(len(feature_columns)) \\\n                                    if feature_columns[i] in must_include_features]\n        must_include_feature_scores = cor_matrix[must_include_feature_ids, -1]\n        must_include_features_ids = [must_include_feature_ids[i] for i in np.argsort(must_include_feature_scores)[::-1]]\n        ranked_feature_ids = must_include_feature_ids\n        remaining_feature_ids = [i for i in range(len(feature_columns)) if i not in must_include_features_ids]\n        remaining_feature_scores = []\n        for feature_id in remaining_feature_ids:\n            remaining_feature_scores.append(cor_matrix[feature_id, -1] - max([cor_matrix[feature_id, i] for i in\n                                                                              ranked_feature_ids]))\n    else:\n        ranked_feature_ids = []\n        remaining_feature_ids = range(len(feature_columns))\n        remaining_feature_scores = cor_matrix[remaining_feature_ids, -1]\n\n    while len(ranked_feature_ids) < len(feature_columns):\n        best_feature_arg = np.argmax(remaining_feature_scores)\n        best_feature_id = remaining_feature_ids[best_feature_arg]\n        remaining_feature_ids = [remaining_feature_ids[i] for i in range(len(remaining_feature_ids)) if i !=\n                                 best_feature_arg]\n        ranked_feature_ids.append(best_feature_id)\n        remaining_feature_scores = []\n        for feature_id in remaining_feature_ids:\n            remaining_feature_scores.append(cor_matrix[feature_id, -1] - max([cor_matrix[feature_id, i] for i in\n                                                                              ranked_feature_ids]))\n    ranked_features = [feature_columns[i] for i in ranked_feature_ids]\n    return ranked_features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb51c4c0-f78b-463f-99ad-cb22887bea07"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.sql import DataFrame\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.feature import VectorAssembler\n\n\ndef feature_selector(df: DataFrame, ranked_features: list, output_column: str, estimator_obj=RandomForestRegressor,\n                     feature_inclusion_increments: int = 1, train_test_split_ratio: list = None, cv: int = -1,\n                     evaluation_metric: str = 'r2'):\n    \"\"\"\n    Trains the estimator at multiple steps, with features progressively added to the input list based on their ranks\n    :param df: the input dataset with features and output as columns\n    :param ranked_features: the output of the feature ranking algorithm or a manually selected ranking scheme\n    :param output_column: the name of the output column in the dataset\n    :param estimator_obj: the training model object\n    :param train_test_split_ratio: the default for train_test_split_ratio is [0.66, 0.33]\n    :param cv: if left as default (c = -1), changes nothing. If selected as a value > 1, it enforces cross validation\n                and overrides the train-test-splitting\n    :param feature_inclusion_increments:\n    :param evaluation_metric: evaluation metric to return for predictions on test set - \"rmse\": root mean\n            squared error - \"mse\": mean squared error - \"r2\" (default): coefficient of determination -\n            \"mae\": mean absolute error\n    \"\"\"\n\n    if train_test_split_ratio is None:\n        train_test_split_ratio = [0.66, 0.33]\n\n    feature_count_list = list(range(1, len(ranked_features), feature_inclusion_increments)) + [len(ranked_features)]\n\n    estimator_features_col = 'features'\n    while estimator_features_col in df.columns:\n        estimator_features_col += '_'\n    estimator_prediction_col = 'prediction'\n    while estimator_prediction_col in df.columns:\n        estimator_prediction_col += '_'\n    estimator_obj.setFeaturesCol(estimator_features_col)\n    estimator_obj.setPredictionCol(estimator_prediction_col)\n    estimator_obj.setLabelCol(output_column)\n\n    evaluator = RegressionEvaluator(\n        labelCol=output_column, predictionCol=estimator_prediction_col, metricName=evaluation_metric)\n\n    scores = []\n    if cv <= 1:\n        df_train, df_test = df.randomSplit(train_test_split_ratio)\n        for feature_count in feature_count_list:\n            input_features = ranked_features[0: feature_count]\n            assembler = VectorAssembler(\n                inputCols=input_features,\n                outputCol=estimator_features_col)\n            df_train = assembler.transform(df_train)\n            fit_model = estimator_obj.fit(df_train)\n            df_test = assembler.transform(df_test)\n            df_test = fit_model.transform(df_test)\n            score = evaluator.evaluate(df_test)\n            scores.append((feature_count, score))\n            df_train = df_train.drop(estimator_features_col)\n            df_test = df_test.drop(estimator_features_col, estimator_prediction_col)\n    else:\n        for feature_count in feature_count_list:\n            input_features = ranked_features[0: feature_count]\n            assembler = VectorAssembler(\n                inputCols=input_features,\n                outputCol=estimator_features_col)\n            df = assembler.transform(df)\n            grid = ParamGridBuilder().addGrid(estimator_obj.featuresCol, [estimator_obj.getFeaturesCol()]).build()\n            crossval = CrossValidator(estimator=estimator_obj,\n                                      evaluator=evaluator,\n                                      numFolds=cv,\n                                      estimatorParamMaps=grid)\n            fit_crossval = crossval.fit(df)\n            scores.append((feature_count, fit_crossval.avgMetrics[0]))\n            df = df.drop(estimator_features_col)\n\n    return scores"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcb959a4-f248-4d1d-b3ef-c61948e8178e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\nfrom sklearn.datasets import load_boston\n\n\n\nfrom pyspark.sql import SparkSession\n\n\n# %% Loading the Boston Dataset as a sample dataset and creating the spark dataframes\ndef test_boston_dataset(spark_session: SparkSession):\n    boston = load_boston()\n    feature_names = boston.feature_names.tolist()\n    output_name = 'outcome'\n    boston_columns = feature_names + [output_name]\n    X = boston.data.tolist()\n    y = boston.target.tolist()\n    Xy = [(i + [j]) for (i, j) in zip(X, y)]\n    boston_df = spark_session.createDataFrame(Xy, boston_columns)\n    print(feature_names)\n    must_include_features = []\n    # must_include_features = ['TAX', 'INDUS']\n\n    # %% Ranking features\n    ranked_features = feature_ranker(df=boston_df,\n                                     feature_columns=feature_names,\n                                     output_column=output_name,\n                                     must_include_features=must_include_features)\n    print(ranked_features)\n    # %% Feature selection\n    scores = feature_selector(df=boston_df,\n                              ranked_features=ranked_features,\n                              output_column=output_name,\n                              estimator_obj=RandomForestRegressor(),\n                              feature_inclusion_increments=1,\n                              train_test_split_ratio=[0.66, 0.33],\n                              cv=-1,\n                              evaluation_metric='r2')\n    return scores"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1652fd4-1b70-4eec-9606-053307f747ff"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["score = test_boston_dataset(spark_session=spark)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94e29308-44a1-41a7-b8d2-f8498b1c64c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n/databricks/spark/python/pyspark/sql/context.py:134: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n['LSTAT', 'PTRATIO', 'RM', 'CHAS', 'B', 'ZN', 'CRIM', 'TAX', 'AGE', 'INDUS', 'NOX', 'DIS', 'RAD']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n/databricks/spark/python/pyspark/sql/context.py:134: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n['LSTAT', 'PTRATIO', 'RM', 'CHAS', 'B', 'ZN', 'CRIM', 'TAX', 'AGE', 'INDUS', 'NOX', 'DIS', 'RAD']\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(score)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80a8cb26-3ab7-491c-a907-d4dcf5c87392"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[(1, 0.6321032889048015), (2, 0.7146604090385802), (3, 0.7821895502958525), (4, 0.7874075309489468), (5, 0.788983942181348), (6, 0.7911891834273723), (7, 0.835318890990556), (8, 0.8220140654821992), (9, 0.7945915217044365), (10, 0.8258071729108915), (11, 0.824370214198534), (12, 0.8293450908069684), (13, 0.8443275163505498)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[(1, 0.6321032889048015), (2, 0.7146604090385802), (3, 0.7821895502958525), (4, 0.7874075309489468), (5, 0.788983942181348), (6, 0.7911891834273723), (7, 0.835318890990556), (8, 0.8220140654821992), (9, 0.7945915217044365), (10, 0.8258071729108915), (11, 0.824370214198534), (12, 0.8293450908069684), (13, 0.8443275163505498)]\n"]}}],"execution_count":0},{"cell_type":"code","source":["top_score = sorted(score, key=lambda item: item[1], reverse=True)[:7]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"279fae38-bca6-4a0c-ac10-5830845fe949"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["index=[]\nfor i in top_score:\n    index.append(i[0])\n\nprint(index)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c22f8437-9ad0-4529-83ff-f3bb27e95e20"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[13, 7, 12, 10, 11, 8, 9]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[13, 7, 12, 10, 11, 8, 9]\n"]}}],"execution_count":0},{"cell_type":"code","source":["top_columns=[]\nfor i in index:\n    top_columns.append(feature_names[i-1])\n    \nprint(top_columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c4ad05b-fd73-4640-b69d-5b0160e4ab95"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['LSTAT', 'AGE', 'B', 'TAX', 'PTRATIO', 'DIS', 'RAD']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['LSTAT', 'AGE', 'B', 'TAX', 'PTRATIO', 'DIS', 'RAD']\n"]}}],"execution_count":0},{"cell_type":"code","source":["boston = load_boston()\nfeature_names = boston.feature_names.tolist()\noutput_name = 'outcome'\nboston_columns = feature_names + [output_name]\nX = boston.data.tolist()\ny = boston.target.tolist()\nXy = [(i + [j]) for (i, j) in zip(X, y)]\nboston_df = spark.createDataFrame(Xy, boston_columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c526a63f-5c21-4f20-b02c-b745af5ce7c6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["top_columns.append(output_name)\nprint(top_columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e698f00e-8383-4ee6-bb07-0308d4eb24ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['LSTAT', 'AGE', 'B', 'TAX', 'PTRATIO', 'DIS', 'RAD', 'outcome']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['LSTAT', 'AGE', 'B', 'TAX', 'PTRATIO', 'DIS', 'RAD', 'outcome']\n"]}}],"execution_count":0},{"cell_type":"code","source":["boston_df = boston_df.selectExpr(top_columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"add462b9-9aef-41b8-b6b1-20cb4d154d03"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["boston_df.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ec312b9-9024-4506-80c6-8a5144774b3c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+----+-----+-----+-------+------+---+-------+\n|LSTAT| AGE|    B|  TAX|PTRATIO|   DIS|RAD|outcome|\n+-----+----+-----+-----+-------+------+---+-------+\n| 4.98|65.2|396.9|296.0|   15.3|  4.09|1.0|   24.0|\n| 9.14|78.9|396.9|242.0|   17.8|4.9671|2.0|   21.6|\n+-----+----+-----+-----+-------+------+---+-------+\nonly showing top 2 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+----+-----+-----+-------+------+---+-------+\n|LSTAT| AGE|    B|  TAX|PTRATIO|   DIS|RAD|outcome|\n+-----+----+-----+-----+-------+------+---+-------+\n| 4.98|65.2|396.9|296.0|   15.3|  4.09|1.0|   24.0|\n| 9.14|78.9|396.9|242.0|   17.8|4.9671|2.0|   21.6|\n+-----+----+-----+-----+-------+------+---+-------+\nonly showing top 2 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["train, test = boston_df.randomSplit([0.7, 0.3], seed=7)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6c92c09-3aad-43e1-bec8-757ac7245bfa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["train.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e99529d-353c-4d87-b0d4-6a1cde202071"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+----+------+-----+-------+------+---+-------+\n|LSTAT| AGE|     B|  TAX|PTRATIO|   DIS|RAD|outcome|\n+-----+----+------+-----+-------+------+---+-------+\n| 1.98|15.8|395.62|252.0|   18.3|5.4011|3.0|   34.9|\n| 2.94|45.8|394.63|222.0|   18.7|6.0622|3.0|   33.4|\n+-----+----+------+-----+-------+------+---+-------+\nonly showing top 2 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+----+------+-----+-------+------+---+-------+\n|LSTAT| AGE|     B|  TAX|PTRATIO|   DIS|RAD|outcome|\n+-----+----+------+-----+-------+------+---+-------+\n| 1.98|15.8|395.62|252.0|   18.3|5.4011|3.0|   34.9|\n| 2.94|45.8|394.63|222.0|   18.7|6.0622|3.0|   33.4|\n+-----+----+------+-----+-------+------+---+-------+\nonly showing top 2 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["catCols = [x for (x, dataType) in train.dtypes if dataType == \"string\"]\nnumCols = [\n    x for (x, dataType) in train.dtypes if ((dataType == \"double\"))\n]\nprint(numCols)\nprint(catCols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc8dea1d-042c-4d0f-bf70-6592eaeb3e69"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['LSTAT', 'AGE', 'B', 'TAX', 'PTRATIO', 'DIS', 'RAD', 'outcome']\n[]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['LSTAT', 'AGE', 'B', 'TAX', 'PTRATIO', 'DIS', 'RAD', 'outcome']\n[]\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nassemblerInput = [x for x in numCols]\nvector_assembler = VectorAssembler(\n    inputCols=assemblerInput, outputCol=\"VectorAssembler_features\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5eb191db-99b2-4c3c-a77a-385a358e3e66"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stages = [vector_assembler]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57505740-f460-446c-8f8a-d3da6974b400"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%%time\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline().setStages(stages)\nmodel = pipeline.fit(train)\npp_df = model.transform(test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a5bde44-b24c-4c0f-948a-3cb563ac6742"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"CPU times: user 5.09 ms, sys: 1.25 ms, total: 6.34 ms\nWall time: 191 ms\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["CPU times: user 5.09 ms, sys: 1.25 ms, total: 6.34 ms\nWall time: 191 ms\n"]}}],"execution_count":0},{"cell_type":"code","source":["pp_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bee3725-ca63-4352-a1aa-7fc2fb989192"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[108]: DataFrame[LSTAT: double, AGE: double, B: double, TAX: double, PTRATIO: double, DIS: double, RAD: double, outcome: double, VectorAssembler_features: vector]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[108]: DataFrame[LSTAT: double, AGE: double, B: double, TAX: double, PTRATIO: double, DIS: double, RAD: double, outcome: double, VectorAssembler_features: vector]"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n\ndata = pp_df.select(\n    F.col(\"VectorAssembler_features\").alias(\"features\"),\n    F.col(\"outcome\").alias(\"label\"),\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3fd570c-546f-4579-a033-f536fa621b3b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data.show(5, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8bf6c142-0204-4204-a051-8fa297748029"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------------------------------------------+-----+\n|features                                     |label|\n+---------------------------------------------+-----+\n|[3.95,40.5,392.9,256.0,15.1,8.3248,5.0,31.6] |31.6 |\n|[4.81,21.9,395.93,226.0,17.9,8.6966,5.0,35.4]|35.4 |\n|[4.98,65.2,396.9,296.0,15.3,4.09,1.0,24.0]   |24.0 |\n|[5.28,21.1,396.9,243.0,16.8,6.8147,4.0,25.0] |25.0 |\n|[5.33,54.2,396.9,222.0,18.7,6.0622,3.0,36.2] |36.2 |\n+---------------------------------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------------------------------------------+-----+\n|features                                     |label|\n+---------------------------------------------+-----+\n|[3.95,40.5,392.9,256.0,15.1,8.3248,5.0,31.6] |31.6 |\n|[4.81,21.9,395.93,226.0,17.9,8.6966,5.0,35.4]|35.4 |\n|[4.98,65.2,396.9,296.0,15.3,4.09,1.0,24.0]   |24.0 |\n|[5.28,21.1,396.9,243.0,16.8,6.8147,4.0,25.0] |25.0 |\n|[5.33,54.2,396.9,222.0,18.7,6.0622,3.0,36.2] |36.2 |\n+---------------------------------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["%%time\nmodel = LinearRegression().fit(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71b97723-ccc9-40e1-8aaf-9f5ef28513d8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"CPU times: user 13.8 ms, sys: 5.84 ms, total: 19.6 ms\nWall time: 931 ms\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["CPU times: user 13.8 ms, sys: 5.84 ms, total: 19.6 ms\nWall time: 931 ms\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(model.summary.meanAbsoluteError)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a97c8f3-b6a6-404b-9731-ad447390a043"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"8.071321389024888e-15\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8.071321389024888e-15\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(model.summary.rootMeanSquaredError)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a406524b-0a89-4b83-a720-d8df7af35e17"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"1.0704760961584674e-14\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["1.0704760961584674e-14\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"sparkml_fs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2732017673410517}},"nbformat":4,"nbformat_minor":0}
